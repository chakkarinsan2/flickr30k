{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chakkarinsan2/flickr30k/blob/main/clip_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TrK7-FSfp5In"
      },
      "outputs": [],
      "source": [
        "# check python version\n",
        "%%bash\n",
        "which python\n",
        "python --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eir8Iz_Ijxut"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "\n",
        "# make sure the conda path is clear so it does not conflict with conda\n",
        "export PYTHONPATH=\"\"\n",
        "\n",
        "# download and install miniconda\n",
        "conda_version='Miniconda3-py37_4.9.2-Linux-x86_64.sh'\n",
        "wget https://repo.anaconda.com/miniconda/${conda_version}\n",
        "chmod +x ${conda_version}\n",
        "./${conda_version} -b -f -p /usr/local\n",
        "\n",
        "# update miniconda\n",
        "conda install --channel defaults conda python=3.7 --yes\n",
        "conda update --channel defaults --all --yes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "frwOuCu0qQsR"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "which python\n",
        "python --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PVqrQo8ca5xv"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "\n",
        "# install dependencies to clip\n",
        "conda install --yes -c pytorch pytorch=1.7.1 torchvision cudatoolkit=11.0\n",
        "pip install ftfy regex tqdm wget\n",
        "\n",
        "# install clip\n",
        "pip install git+https://github.com/openai/CLIP.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "srZqNZjYTqYe"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "# make sure the conda libraries are recognized here\n",
        "_ = sys.path.append(\"/usr/local/lib/python3.7/site-packages\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iU1InqHqU8X-"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import clip\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# load model and image preprocessing\n",
        "model, preprocess = clip.load(\"ViT-B/32\", device=device, jit=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d8RJ8wcfM3Sb"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# สร้างโฟลเดอร์ชื่อ xxx เพื่อ mount drive จาก google drive ให้ไปทำงานใน google colab\n",
        "!mkdir flickr30k\n",
        "\n",
        "# โหลด zip file รูปภาพ\n",
        "if not Path('flickr30k/flickr30k_images01.zip').exists():\n",
        "  !wget https://github.com/chakkarinsan2/flickr30k/releases/download/training/flickr30k_images01.zip -O flickr30k/flickr30k_images01.zip\n",
        "\n",
        "if not Path('flickr30k/flickr30k_images02.zip').exists():\n",
        "  !wget https://github.com/chakkarinsan2/flickr30k/releases/download/training/flickr30k_images02.zip -O flickr30k/flickr30k_images02.zip\n",
        "\n",
        "if not Path('flickr30k/flickr30k_images03.zip').exists():\n",
        "  !wget https://github.com/chakkarinsan2/flickr30k/releases/download/training/flickr30k_images03.zip -O flickr30k/flickr30k_images03.zip\n",
        "\n",
        "# โหลดไฟล์ caption แบบ csv\n",
        "if not Path('flickr30k/results_revised.csv').exists():\n",
        "  !wget https://github.com/chakkarinsan2/flickr30k/releases/download/training/results_revised.csv -O flickr30k/results_revised.csv\n",
        "\n",
        "# โหลดไฟล์คุณลักษณะรูปภาพของ image_feature.npy (haltakov, 2021)\n",
        "if not Path('flickr30k/image_features.npy').exists():\n",
        "  !wget https://github.com/chakkarinsan2/flickr30k/releases/download/training/image_features.npy -O flickr30k/image_features.npy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "InUjUIqbTsnV"
      },
      "outputs": [],
      "source": [
        "# แตก zip รูปภาพลงในโฟลเดอร์  flickr30k_images\n",
        "\n",
        "!unzip \"/content/flickr30k/flickr30k_images01.zip\" -d \"/content/flickr30k/flickr30k_images\"\n",
        "\n",
        "!unzip \"/content/flickr30k/flickr30k_images02.zip\" -d \"/content/flickr30k/flickr30k_images\"\n",
        "\n",
        "!unzip \"/content/flickr30k/flickr30k_images03.zip\" -d \"/content/flickr30k/flickr30k_images\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# กำหนด path ของโฟลเดอร์ flickr30k ในการเรียกใช้ในครั้งต่อๆไป\n",
        "path = '/content/flickr30k/flickr30k_images'"
      ],
      "metadata": {
        "id": "dee8aSH51EZ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F3SiW0Lv8ipT"
      },
      "outputs": [],
      "source": [
        "# ตรวจสอบจำนวนรูปภาพในโฟลเดอร์ (=30000)\n",
        "import os\n",
        "len(os.listdir(path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9__zxx7kpGfZ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# แสดงจำนวนรายการ caption ใน results.csv\n",
        "df = pd.read_csv('/content/flickr30k/results_revised.csv', header=None, sep='|')\n",
        "df = df[0].str.split(',', expand=True)\n",
        "\n",
        "print(df.shape)\n",
        "\n",
        "df.head(145541)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ตรวจสอบรายชื่อรูปภาพใน path\n",
        "data = []\n",
        "\n",
        "for filename in os.listdir(path):\n",
        "    if filename.endswith(\"jpg\"):\n",
        "        # แสดงรายชื่อรูปภาพ\n",
        "        print(filename)\n",
        "        data.append(filename)"
      ],
      "metadata": {
        "id": "5k12IVXp0uxT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training Model**"
      ],
      "metadata": {
        "id": "gEr0mmdekGlW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install ftfy regex tqdm\n",
        "! pip install git+https://github.com/openai/CLIP.git"
      ],
      "metadata": {
        "id": "3b42PGtYsaMt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from pkg_resources import packaging\n",
        "\n",
        "print(\"Torch version:\", torch.__version__)"
      ],
      "metadata": {
        "id": "ooynknKeo8Xz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import clip\n",
        "\n",
        "clip.available_models()"
      ],
      "metadata": {
        "id": "98uBv16do9cr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model, preprocess = clip.load(\"ViT-B/32\")\n",
        "input_resolution = model.visual.input_resolution\n",
        "context_length = model.context_length\n",
        "vocab_size = model.vocab_size\n",
        "\n",
        "print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\n",
        "print(\"Input resolution:\", input_resolution)\n",
        "print(\"Context length:\", context_length)\n",
        "print(\"Vocab size:\", vocab_size)"
      ],
      "metadata": {
        "id": "pojUY3l6pBnU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get full dataset (image+text)\n",
        "import os.path\n",
        "from typing import Dict, Tuple\n",
        "\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "def get_full_dataset(\n",
        "    batch_size: int = 32, image_size: Tuple[int, int] = (256, 256)\n",
        ") -> tf.data.Dataset:\n",
        "    data = pd.read_csv(os.path.join(DATA_ABS_PATH, \"/content/flickr30k/results_revised.csv\"))\n",
        "    images_path = os.path.join(DATA_ABS_PATH, \"/content/flickr30k/flickr30k_images/\")\n",
        "    data[\"image\"] = data[\"image\"].map(lambda x: os.path.join(images_path, f\"{x}.jpg\"))\n",
        "    filenames: tf.Tensor = tf.constant(data[\"image\"], dtype=tf.string)\n",
        "    data[\"label\"] = data[\"label\"].str.lower()\n",
        "    class_name_to_label: Dict[str, int] = {\n",
        "        label: i for i, label in enumerate(set(data[\"label\"]))\n",
        "    }\n",
        "    labels: tf.Tensor = tf.constant(\n",
        "        data[\"label\"].map(class_name_to_label.__getitem__), dtype=tf.uint8\n",
        "    )\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((filenames, labels))\n",
        "\n",
        "    def _parse_function(filename, label):\n",
        "        jpg_image: tf.Tensor = tf.io.decode_jpeg(tf.io.read_file(filename))\n",
        "        return tf.image.resize(jpg_image, size=image_size), label\n",
        "\n",
        "    dataset = dataset.map(_parse_function)\n",
        "    return dataset.batch(batch_size)\n"
      ],
      "metadata": {
        "id": "eCkC30aH__Ly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Image Preprocessing**\n",
        "\n",
        "data augmentation and analysis from clip.load() contains a torchvision Transform that performs this preprocessing."
      ],
      "metadata": {
        "id": "GQJX44uqc_E-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preprocess"
      ],
      "metadata": {
        "id": "o9uHrr6GNkq3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Text Preprocessing**\n",
        "\n",
        "using clip.tokenize(), the outputs are padded to become 77 tokens long, which is what the CLIP models expects."
      ],
      "metadata": {
        "id": "V0TUvo9OZb-r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# split csv column\n",
        "data = pd.read_csv(\"/content/flickr30k/results_revised.csv\", delimiter=\",\",header=None) # split by ,\n",
        "\n",
        "print(data[2]) # only column[2] = caption\n",
        "\n",
        "# text_enceode\n",
        "clip.tokenize(data[2])"
      ],
      "metadata": {
        "id": "Y7XgP11GSm9L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Setting up input images and texts**\n",
        "\n",
        "input images and text descriptions to the model, and compare the similarity between the corresponding features."
      ],
      "metadata": {
        "id": "DHf7nMRmkgvu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import skimage\n",
        "import IPython.display\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "from collections import OrderedDict\n",
        "import torch\n",
        "\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "\n",
        "description = {print(data[2])}\n",
        "descriptions = {\"page\": \"a page of text about segmentation\"}"
      ],
      "metadata": {
        "id": "J1OVgj1wgSST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# แสดงรูปภาพทั้งหมด\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "%matplotlib inline\n",
        "\n",
        "images = []\n",
        "for img_path in glob.glob('/content/flickr30k/flickr30k_images/*.jpg'):\n",
        "    images.append(mpimg.imread(img_path))\n",
        "\n",
        "plt.figure(figsize=(20,10))\n",
        "columns = 5\n",
        "for i, image in enumerate(images):\n",
        "    plt.subplot(len(images) / columns + 1, columns, i + 1)\n",
        "    plt.imshow(image)\n",
        "\n",
        "original_images = []\n",
        "images = []\n",
        "texts = []\n",
        "plt.figure(figsize=(16, 5))\n",
        "\n",
        "for filename in [filename for filename in os.listdir(skimage.data_dir) if filename.endswith(\".png\") or filename.endswith(\".jpg\")]:\n",
        "    name = os.path.splitext(filename)[0]\n",
        "    if name not in descriptions:\n",
        "        continue\n",
        "\n",
        "    image = Image.open(os.path.join(skimage.data_dir, filename)).convert(\"RGB\")\n",
        "\n",
        "    original_images.append(image)\n",
        "    images.append(preprocess(image))\n",
        "    texts.append(descriptions[name])\n",
        "\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "xQDsKRo1i2Am"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_input = torch.tensor(np.stack(images))\n",
        "text_tokens = clip.tokenize([\"This is \" + desc for desc in texts])"
      ],
      "metadata": {
        "id": "IN1lD-AOlgP7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    image_features = model.encode_image(image_input).float()\n",
        "    text_features = model.encode_text(text_tokens).float()"
      ],
      "metadata": {
        "id": "voCxhtFwvxC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Calculating cosine similarity**\n",
        "\n",
        "between the features and calculate the dot product of each pair."
      ],
      "metadata": {
        "id": "q9_57xD9jmf7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "similarity = text_features.cpu().numpy() @ image_features.cpu().numpy().T"
      ],
      "metadata": {
        "id": "S5RSsqK7jhDq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Testing**"
      ],
      "metadata": {
        "id": "h_muLK1f2bNN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XIIX7mFvVHaV"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "\n",
        "# load image โดยสุ่มเลือกรูปมาจาก google drive\n",
        "image = Image.open(r\"/content/flickr30k/flickr30k_images/1243756.jpg\")\n",
        "print(\"Image to be processed\")\n",
        "display(image)\n",
        "\n",
        "# pre-process image\n",
        "image = preprocess(image).unsqueeze(0).to(device)\n",
        "print(\"\\n\\nTensor shape:\")\n",
        "print(image.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M1qrlJd6bl_6"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    image_features = model.encode_image(image)\n",
        "print(image_features.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ksDpn6n1VX1r"
      },
      "outputs": [],
      "source": [
        "# นำเข้า caption จาก csv เพื่อทดสอบการเรียนรู้ของ model ว่าถูกต้องเพียงใด 5 caption per image\n",
        "text_snippets = [\"A man is sitting in a chair in front of a Ben and Jerry 's machine\", \"A very unusually dressed man sitting beside an ice cream cooler\", \"One person wearing a coat and hat sitting in a chair\", \"A man in strange outfit sits in a lawn chair near a Ben and Jerry 's stand\", \"Man sitting in a chair wearing a hat and scarf\"]\n",
        "\n",
        "# pre-process text\n",
        "text = clip.tokenize(text_snippets).to(device)\n",
        "print(text.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ro4rEZCJVxbP"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    text_features = model.encode_text(text)\n",
        "print(text_features.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FK87afI2I5Wq"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    logits_per_image, logits_per_text = model(image, text)\n",
        "    probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
        "\n",
        "print(\"ค่าความน่าจะเป็นในการพยากรณ์:\", probs)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}